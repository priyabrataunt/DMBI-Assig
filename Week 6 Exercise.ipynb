{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "vyrxE8fkgBJK"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch . nn as nn\n",
        "import torch . nn . functional as F\n",
        "import torchvision\n",
        "import torchvision . transforms as T\n",
        "from torch . utils . data import DataLoader , Dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SimCLRDataset(Dataset):\n",
        "    def __init__(self, dataset, transform):\n",
        "        self.dataset = dataset\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x, _ = self.dataset[idx]        # Ignore labels!\n",
        "        x1 = self.transform(x)          # First augmentation\n",
        "        x2 = self.transform(x)          # Second augmentation\n",
        "        return x1, x2\n",
        "\n",
        "# Strong augmentations for SimCLR (CIFAR-10)\n",
        "simclr_transform = T.Compose([\n",
        "    T.RandomResizedCrop(32, scale=(0.2, 1.0)),\n",
        "    T.RandomHorizontalFlip(),\n",
        "    T.RandomApply([T.ColorJitter(0.4,0.4,0.4,0.1)], p=0.8),\n",
        "    T.RandomGrayscale(p=0.2),\n",
        "    T.GaussianBlur(3, sigma=(0.1, 2.0)),\n",
        "    T.ToTensor(),\n",
        "])"
      ],
      "metadata": {
        "id": "9caUaI9AgiwP"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.AdaptiveAvgPool2d((1, 1))\n",
        "        )\n",
        "        self.fc = nn.Linear(256, 128)   # representation size\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.conv(x)\n",
        "        h = h.view(h.size(0), -1)\n",
        "        h = self.fc(h)\n",
        "        return F.normalize(h, dim=1)"
      ],
      "metadata": {
        "id": "DS4GPmYhgnnq"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ProjectionHead(nn.Module):\n",
        "    def __init__(self, in_dim=128, out_dim=128):\n",
        "        super().__init__()\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(in_dim, in_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(in_dim, out_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return F.normalize(self.mlp(x), dim=1)"
      ],
      "metadata": {
        "id": "fb3ApC3Ag7uZ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def nt_xent_loss(z1, z2, temperature=0.5):\n",
        "    N = z1.size(0)\n",
        "    z = torch.cat([z1, z2], dim=0)   # Concatenate: 2N x d\n",
        "\n",
        "    # Compute similarity matrix\n",
        "    sim = torch.mm(z, z.t()) / temperature\n",
        "\n",
        "    # Mask out self-similarities (diagonal)\n",
        "    mask = torch.eye(2*N, dtype=torch.bool, device=z.device)\n",
        "    sim = sim.masked_fill(mask, -9e15)\n",
        "\n",
        "    # Create positive pair labels\n",
        "    positives = torch.cat([torch.arange(N, 2*N), torch.arange(0, N)]).to(z.device)\n",
        "    labels = positives\n",
        "\n",
        "    # Cross-entropy loss\n",
        "    loss = F.cross_entropy(sim, labels)\n",
        "    return loss"
      ],
      "metadata": {
        "id": "lGi9HYeHhEqy"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c9803c6d",
        "outputId": "e94a7f49-81db-48e0-8a1e-9ad3f9d334c5"
      },
      "source": [
        "# Load CIFAR-10 dataset\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True)\n",
        "\n",
        "# Create SimCLR dataset\n",
        "simclr_train_dataset = SimCLRDataset(train_dataset, simclr_transform)\n",
        "\n",
        "# Create DataLoader\n",
        "train_loader = DataLoader(simclr_train_dataset, batch_size=256, shuffle=True, num_workers=2)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:02<00:00, 58.3MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
        "encoder = Encoder().to(device)\n",
        "projector = ProjectionHead().to(device)\n",
        "optimizer = torch.optim.Adam(list(encoder.parameters()) + list(projector.parameters()), lr=1e-3)\n",
        "\n",
        "def train_simclr():\n",
        "    encoder.train()\n",
        "    projector.train()\n",
        "    for epoch in range(10): # Quick demo\n",
        "        total_loss = 0\n",
        "        for batch_idx, (x1, x2) in enumerate(train_loader):\n",
        "            x1, x2 = x1.to(device), x2.to(device)\n",
        "            # Forward pass\n",
        "            h1, h2 = encoder(x1), encoder(x2)\n",
        "            z1, z2 = projector(h1), projector(h2)\n",
        "            # Compute loss\n",
        "            loss = nt_xent_loss(z1, z2)\n",
        "            # Backward pass\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        print(f\"Epoch {epoch + 1}: Loss = {total_loss / len(train_loader):.4f}\")\n",
        "\n",
        "    # Save the encoder (discard projector)\n",
        "    torch.save(encoder.state_dict(), \"simclr_encoder.pth\")\n",
        "\n",
        "train_simclr()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Jv_MIFsk34X",
        "outputId": "808ca323-7e8c-4d6d-f212-fabd4603c700"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Loss = 5.5854\n",
            "Epoch 2: Loss = 5.3048\n",
            "Epoch 3: Loss = 5.2154\n",
            "Epoch 4: Loss = 5.1548\n",
            "Epoch 5: Loss = 5.1369\n",
            "Epoch 6: Loss = 5.1093\n",
            "Epoch 7: Loss = 5.0900\n",
            "Epoch 8: Loss = 5.0754\n",
            "Epoch 9: Loss = 5.0657\n",
            "Epoch 10: Loss = 5.0472\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pre - trained encoder\n",
        "encoder = Encoder ()\n",
        "encoder . load_state_dict ( torch . load ( \"simclr_encoder.pth\" ) )\n",
        "class FCNClassifier ( nn . Module ) :\n",
        "    def __init__ ( self , encoder ) :\n",
        "        super () . __init__ ()\n",
        "        self . encoder = encoder\n",
        "        # Freeze encoder parameters\n",
        "        for param in self . encoder . parameters () :\n",
        "            param . requires_grad = False\n",
        "        self . fc = nn . Linear (128 , 10) # CIFAR -10 has 10 classes\n",
        "    def forward ( self , x ) :\n",
        "        h = self . encoder ( x ) # Frozen features\n",
        "        return self . fc ( h ) # Only train classifier\n",
        "# Create limited labeled dataset (10% per class )\n",
        "def create_limited_dataset(dataset, samples_per_class=500):\n",
        "    limited_indices = []\n",
        "    targets = dataset.targets if hasattr(dataset, 'targets') else dataset._targets # Handle different dataset types\n",
        "\n",
        "    # Get indices for each class\n",
        "    class_indices = {}\n",
        "    for i, target in enumerate(targets):\n",
        "        if target not in class_indices:\n",
        "            class_indices[target] = []\n",
        "        class_indices[target].append(i)\n",
        "\n",
        "    # Select a limited number of samples per class\n",
        "    for class_idx, indices in class_indices.items():\n",
        "        num_samples = min(samples_per_class, len(indices))\n",
        "        limited_indices.extend(torch.randperm(len(indices))[:num_samples].tolist())\n",
        "\n",
        "    # Create a subset of the original dataset\n",
        "    limited_dataset = torch.utils.data.Subset(dataset, limited_indices)\n",
        "    return limited_dataset\n",
        "\n",
        "# Assuming 'train_dataset' is already defined and loaded\n",
        "# For example, if using CIFAR10:\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True)\n",
        "\n",
        "limited_dataset = create_limited_dataset(train_dataset, samples_per_class=500)\n",
        "classifier = FCNClassifier ( encoder ) . to ( device )"
      ],
      "metadata": {
        "id": "ViKyqKZaRYLM"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fa50108f",
        "outputId": "2005a301-f948-429d-dd61-b38ddb9f5bae"
      },
      "source": [
        "# Create DataLoader for the limited dataset\n",
        "# Define transformation for the classifier\n",
        "classifier_transform = T.Compose([\n",
        "    T.ToTensor(),\n",
        "    T.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)), # CIFAR-10 normalization\n",
        "])\n",
        "\n",
        "# Apply transformation to the limited dataset\n",
        "class ClassifierDataset(Dataset):\n",
        "    def __init__(self, dataset, transform=None):\n",
        "        self.dataset = dataset\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x, y = self.dataset[idx]\n",
        "        if self.transform:\n",
        "            x = self.transform(x)\n",
        "        return x, y\n",
        "\n",
        "limited_classifier_dataset = ClassifierDataset(limited_dataset, transform=classifier_transform)\n",
        "limited_train_loader = DataLoader(limited_classifier_dataset, batch_size=64, shuffle=True, num_workers=0)\n",
        "\n",
        "# Define loss function and optimizer for the classifier\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer_classifier = torch.optim.Adam(classifier.fc.parameters(), lr=1e-3)\n",
        "\n",
        "def train_classifier():\n",
        "    classifier.train()\n",
        "    for epoch in range(10): # Train for a few epochs\n",
        "        total_loss = 0\n",
        "        for batch_idx, (inputs, targets) in enumerate(limited_train_loader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = classifier(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            # Backward pass\n",
        "            optimizer_classifier.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer_classifier.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        print(f\"Classifier Epoch {epoch + 1}: Loss = {total_loss / len(limited_train_loader):.4f}\")\n",
        "\n",
        "train_classifier()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classifier Epoch 1: Loss = 1.8435\n",
            "Classifier Epoch 2: Loss = 1.8317\n",
            "Classifier Epoch 3: Loss = 1.8211\n",
            "Classifier Epoch 4: Loss = 1.8081\n",
            "Classifier Epoch 5: Loss = 1.7950\n",
            "Classifier Epoch 6: Loss = 1.7879\n",
            "Classifier Epoch 7: Loss = 1.7808\n",
            "Classifier Epoch 8: Loss = 1.7750\n",
            "Classifier Epoch 9: Loss = 1.7703\n",
            "Classifier Epoch 10: Loss = 1.7678\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5745ab5c",
        "outputId": "a24b5ba0-fccc-4c05-fe9e-20398a145c85"
      },
      "source": [
        "# Load CIFAR-10 test dataset\n",
        "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True)\n",
        "\n",
        "# Apply the same transformation as the classifier training data\n",
        "test_classifier_dataset = ClassifierDataset(test_dataset, transform=classifier_transform)\n",
        "\n",
        "# Create DataLoader for the test set\n",
        "test_loader = DataLoader(test_classifier_dataset, batch_size=64, shuffle=False, num_workers=0)\n",
        "\n",
        "def evaluate_classifier():\n",
        "    classifier.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in test_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = classifier(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += targets.size(0)\n",
        "            correct += (predicted == targets).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f\"Test Accuracy of the classifier on the 10000 test images: {accuracy:.2f}%\")\n",
        "\n",
        "evaluate_classifier()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy of the classifier on the 10000 test images: 34.62%\n"
          ]
        }
      ]
    }
  ]
}